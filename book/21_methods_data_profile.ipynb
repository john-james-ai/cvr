{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Profile\n",
    "This first examination of the data seeks to characterize data quality in its (near) raw form. Here, we will discover the scope and breadth of data preprocessing that would be required before advancing to the exploratory analysis effort. The remainder of this section is organized as follows:\n",
    "\n",
    "1. Data Acquisition and Ingestion    \n",
    "    1.0. Extract: Extract data from the source site.    \n",
    "    1.1. Transform: Preliminary preprocessing prior to loading and analysis.     \n",
    "    1.2. Load: Load the data into Dataset objects.    \n",
    "    \n",
    "2. Data Profile    \n",
    "    2.0. Descriptive statistics, missing values and cardinality.    \n",
    "    2.1. Distribution analysis of continuous variables.    \n",
    "    2.2. Frequency analysis of categorical variables.     \n",
    "    \n",
    "8. Summary and Recommendations: Characterize key findings and data preprocessing recommendations.\n",
    "\n",
    "## Data Acquisition and Ingestion\n",
    "Extract, transform, load (ETL), popularized in the ’70s, is a design pattern for continuous extraction of data from multiple heterogeneous data sources, enforcement of strict data quality and consistency standards, and finally, the presentation of massive, transformed, normalized, and sanitized data into enterprise-level data warehouses and (or) data lakes, for analysis, business intelligence, and analytics. Though this project has none of those things, we’ll adopt ETL as our organizing framework for the data acquisition phase.\n",
    "\n",
    "### Extract Transform Load Data Pipeline\n",
    "To orchestrate the ETL process, we'll need to construct a DataPipeline object, and a series of Tasks to perform the operations. The necessary modules are described here. \n",
    "\n",
    "| Step | Class         | Description                                                             |\n",
    "|------|---------------|-------------------------------------------------------------------------|\n",
    "| 1    | Download      | Downloads the source data into a local directory                        |\n",
    "| 2    | Decompress    | Decompresses the data from a gzip archive                               |\n",
    "| 3    | Copy          | Copies the data to an immutable raw data file.                          |\n",
    "| 4    | ConvertDtypes | Converts the target and object variables to category data type.         |\n",
    "| 5    | SetNA         | Changes the missing value indicator, '-1', to NaNs.                     |\n",
    "| 6    | BuildDataset  | Constructs a Dataset object from the data.                              |\n",
    "| 7    | Dataset       | Object encapsulating the data and various behaviors used for profiling. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvr.core.pipeline import DataPipeline, DataPipelineBuilder, PipelineCommand\n",
    "from cvr.core.task import Download, Decompress, Copy, ConvertDtypes, SetNA, SavePKLDataFrame\n",
    "from cvr.utils.config import CriteoConfig \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Pipeline\n",
    "##### Configuration\n",
    "We begin with the CriteoConfig object which packages the information about the Criteo data source such as:\n",
    "\n",
    "- url: Web address for the Criteo Sponsored Search Conversion Log Dataset   \n",
    "- destination: Local filepath where the data will be downloaded\n",
    "- filepath_decompressed: Criteo data filepath for uncompressed version.    \n",
    "- filepath_raw\tRaw data filepath   \n",
    "- workspace: The workspace into which the final Dataset object will be stored.\n",
    "\n",
    "For illustration purposes, the configuration on this machine is a follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        Criteo Data Source Configuration                        \n",
      "                        ________________________________                        \n",
      "                      name : Criteo Sponsored Search Conversion Log Dataset\n",
      "                       url : http://go.criteo.net/criteo-research-search-conversion.tar.gz\n",
      "               destination : tests/test_data/criteo/external/criteo.tar.gz\n",
      "     filepath_decompressed : tests/test_data/criteo/external/Criteo_Conversion_Search/CriteoSearchData\n",
      "              filepath_raw : tests/test_data/criteo/raw/criteo.csv\n",
      "                 workspace : root\n",
      "                       sep : \\t\n",
      "                   missing : -1\n"
     ]
    }
   ],
   "source": [
    "config_filepath = \"tests\\\\test_config\\criteo.yaml\"\n",
    "config = CriteoConfig(config_filepath=config_filepath)\n",
    "config.print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tasks\n",
    "The following Task objects comprise the ETL pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "download = Download(source=config.url, destination=config.destination)\n",
    "decompress = Decompress(source=config.destination, destination=config.filepath_decompressed)\n",
    "copy = Copy(source=config.filepath_decompressed, destination=config.filepath_raw)\n",
    "# Transform\n",
    "convert = ConvertDtypes(source=config.filepath_raw)\n",
    "setna = SetNA(value=[-1,\"-1\"])\n",
    "# Load\n",
    "#builder = BuildDataset()\n",
    "#load = LoadDataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Pipeline\n",
    "Next, we construct the data pipeline and a command object to parameterize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = PipelineCommand(name=\"Criteo ETL\", force=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataPipelineBuilder()\n",
    "builder.create(command)\n",
    "builder.add_task(download)\n",
    "builder.add_task(decompress)\n",
    "builder.add_task(copy)\n",
    "builder.add_task(convert)\n",
    "builder.add_task(setna)\n",
    "builder.build()\n",
    "pipeline = builder.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is ready to run. The raw dataset is approximately 6.5 Gb, so this will take approximately __ minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 05:31:41,104 - root_pipeline - INFO - Started root_pipeline\n",
      "2022-01-21 05:31:41,105 - root_pipeline - INFO - Started Task: Download\n",
      "2022-01-21 05:31:41,953 - root_pipeline - INFO - \tDownloading 1910.08 Mb\n",
      "\n",
      "2022-01-21 05:32:00,835 - root_pipeline - INFO - \tChunk #10: 5.24 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:32:19,435 - root_pipeline - INFO - \tChunk #20: 10.47 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:32:37,856 - root_pipeline - INFO - \tChunk #30: 15.71 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:32:56,721 - root_pipeline - INFO - \tChunk #40: 20.94 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:33:15,329 - root_pipeline - INFO - \tChunk #50: 26.18 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:33:34,072 - root_pipeline - INFO - \tChunk #60: 31.41 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:33:53,035 - root_pipeline - INFO - \tChunk #70: 36.65 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:34:11,422 - root_pipeline - INFO - \tChunk #80: 41.88 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:34:30,435 - root_pipeline - INFO - \tChunk #90: 47.12 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:34:49,005 - root_pipeline - INFO - \tChunk #100: 52.35 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:35:07,722 - root_pipeline - INFO - \tChunk #110: 57.59 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:35:26,343 - root_pipeline - INFO - \tChunk #120: 62.82 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:35:45,139 - root_pipeline - INFO - \tChunk #130: 68.06 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:36:03,725 - root_pipeline - INFO - \tChunk #140: 73.3 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:36:22,503 - root_pipeline - INFO - \tChunk #150: 78.53 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:36:41,088 - root_pipeline - INFO - \tChunk #160: 83.77 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:37:00,192 - root_pipeline - INFO - \tChunk #170: 89.0 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:37:18,868 - root_pipeline - INFO - \tChunk #180: 94.24 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:37:37,688 - root_pipeline - INFO - \tChunk #190: 99.47 percent downloaded at 5 Mbps\n",
      "2022-01-21 05:37:39,471 - root_pipeline - INFO - Ended Task: Download. Status: 200: OK\n",
      "2022-01-21 05:37:39,473 - root_pipeline - INFO - Started Task: Decompress\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        DataPipeline Criteo ETL Summary                         \n",
      "                                 Download Step                                  \n",
      "                        _______________________________                         \n",
      "                            Status Code : 200\n",
      "                           Content Type : application/x-gzip\n",
      "                          Last Modified : Wed, 08 Apr 2020 13:39:53 GMT\n",
      "                    Content Length (Mb) : 1,910.081\n",
      "                        Chunk Size (Mb) : 10\n",
      "                      Chunks Downloaded : 193\n",
      "                        Downloaded (Mb) : 1,910.081\n",
      "                         File Size (Mb) : 1,910.081\n",
      "                                   Mbps : 5.33\n",
      "                                  Start : 2022-01-21 05:31:41.107671\n",
      "                                    End : 2022-01-21 05:37:39.471976\n",
      "                               Duration : 0:05:58.364305\n",
      "                                 Status : 200: OK\n",
      "                            Status Date : 2022-01-21\n",
      "                            Status Time : 05:37:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 05:39:09,846 - root_pipeline - INFO - Ended Task: Decompress. Status: 200: OK\n",
      "2022-01-21 05:39:09,849 - root_pipeline - INFO - Started Task: Copy\n",
      "2022-01-21 05:39:09,852 - root_pipeline - INFO - Ended Task: Copy. Status: 215: Complete - Not Executed: Output Data Already Exists\n",
      "2022-01-21 05:39:09,853 - root_pipeline - INFO - Started Task: ConvertDtypes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        DataPipeline Criteo ETL Summary                         \n",
      "                                Decompress Step                                 \n",
      "                        _______________________________                         \n",
      "                          Source : tests/test_data/criteo/external/criteo.tar.gz\n",
      "                 Compressed Size : 2,002,864,638\n",
      "                     Destination : tests/test_data/criteo/external/Criteo_Conversion_Search/CriteoSearchData\n",
      "                   Expanded Size : 6,426,808,162\n",
      "                           Start : 2022-01-21 05:37:39.473976\n",
      "                             End : 2022-01-21 05:39:09.846819\n",
      "                        Duration : 0:01:30.372843\n",
      "                          Status : 200: OK\n",
      "                     Status Date : 2022-01-21\n",
      "                     Status Time : 05:39:09\n",
      "\n",
      "\n",
      "                        DataPipeline Criteo ETL Summary                         \n",
      "                                   Copy Step                                    \n",
      "                        _______________________________                         \n",
      "                     Source : tests/test_data/criteo/external/Criteo_Conversion_Search/CriteoSearchData\n",
      "                Destination : tests/test_data/criteo/raw/criteo.csv\n",
      "                      Start : 2022-01-21 05:39:09.849856\n",
      "                        End : 2022-01-21 05:39:09.851848\n",
      "                   Duration : 0:00:00.001992\n",
      "                     Status : 215: Complete - Not Executed: Output Data Already Exists\n",
      "                Status Date : 2022-01-21\n",
      "                Status Time : 05:39:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 05:43:53,338 - root_pipeline - INFO - Ended Task: ConvertDtypes. Status: 200: OK\n",
      "2022-01-21 05:43:53,342 - root_pipeline - INFO - Started Task: SetNA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        DataPipeline Criteo ETL Summary                         \n",
      "                               ConvertDtypes Step                               \n",
      "                        _______________________________                         \n",
      "                                   Rows : 15,995,634\n",
      "                                Columns : 23\n",
      "                                   Size : 5,380,123,480\n",
      "                                Missing : 65,417\n",
      "                        Rows w/ Missing : 65,417\n",
      "                     Columns w/ Missing : 1\n",
      "                               category : 17\n",
      "                                  int64 : 3\n",
      "                                float64 : 2\n",
      "                                 object : 1\n",
      "                                  Start : 2022-01-21 05:39:09.853834\n",
      "                                    End : 2022-01-21 05:43:53.338569\n",
      "                               Duration : 0:04:43.484735\n",
      "                                 Status : 200: OK\n",
      "                            Status Date : 2022-01-21\n",
      "                            Status Time : 05:43:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 05:44:09,037 - root_pipeline - INFO - Ended Task: SetNA. Status: 200: OK\n",
      "2022-01-21 05:44:09,040 - root_pipeline - INFO - Completed root_pipeline. Duration 0:12:27.935732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        DataPipeline Criteo ETL Summary                         \n",
      "                                   SetNA Step                                   \n",
      "                        _______________________________                         \n",
      "                                   Rows : 15,995,634\n",
      "                                Columns : 23\n",
      "                                  Cells : 367,899,582\n",
      "                         Missing Before : 65,417\n",
      "                       Missing % Before : 0.02\n",
      "                          Missing After : 167,746,417\n",
      "                        Missing % After : 45.6\n",
      "                               % Change : 256,326.34\n",
      "                                  Start : 2022-01-21 05:43:53.342629\n",
      "                                    End : 2022-01-21 05:44:09.037405\n",
      "                               Duration : 0:00:15.694776\n",
      "                                 Status : 200: OK\n",
      "                            Status Date : 2022-01-21\n",
      "                            Status Time : 05:44:09\n"
     ]
    }
   ],
   "source": [
    "pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba6615e5a7b7e3657be37061ca9c48b0632f150281e10ebc62a462ee17016418"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('cvr': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
