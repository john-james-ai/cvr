{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Profile\n",
    "This first examination of the data seeks to characterize data quality in its (near) raw form. Here, we will discover the scope and breadth of data preprocessing that will be considered before advancing to the exploratory analysis effort. The remainder of this section is organized as follows:\n",
    "\n",
    "   1. Descriptive statistics, missing values and cardinality.    \n",
    "   2. Distribution analysis of continuous variables.    \n",
    "   3. Frequency analysis of categorical variables.         \n",
    "   4. Summary and recommendations\n",
    "\n",
    "If you recall, in the last section, we loaded the Dataset object, vesuvio, into the staging area of the workspace of the same name.  Let's instantiate vesuvio (singleton) and obtain the Datasaet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myst_nb import glue\n",
    "from cvr.core.workspace import Workspace\n",
    "import pandas as pd\n",
    "#pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.width', 1000)\n",
    "random_state = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                                Dataset Summary                                 \n",
      "                                 staging_greco                                  \n",
      "                                _______________                                 \n",
      "                                   Rows : 10,000\n",
      "                                Columns : 23\n",
      "                          Missing Cells : 113,984\n",
      "                        Missing Cells % : 49.56\n",
      "                         Duplicate Rows : 0\n",
      "                       Duplicate Rows % : 0.0\n",
      "                              Size (Mb) : 3.94\n"
     ]
    }
   ],
   "source": [
    "workspace = Workspace('vesuvio')\n",
    "dataset = workspace.get_dataset(stage='staging', name='vesuvio')\n",
    "summary = dataset.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "rows"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "columns"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "49.56"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "missing"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.94"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "size"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "dups"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = glue(\"rows\",summary[\"Rows\"])\n",
    "_ = glue(\"columns\", summary[\"Columns\"])\n",
    "_ = glue(\"missing\", summary[\"Missing Cells %\"])\n",
    "_ = glue(\"size\", summary[\"Size (Mb)\"])\n",
    "_ = glue(\"dups\", summary[\"Duplicate Rows\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains some {glue:}`rows` observations, each with {glue:}`columns` columns for a size of {glue:}`size` Mb.  Some {glue:}`missing`% of the data are missing which reflects the sparsity of user behavior logs. Further, we have some {glue:}`dups` duplicate rows which, one might consider a large number, although that makes up less than 0.2 percent of the sample. Let's take a look at a few sample observations from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                                     greco                                      \n",
      "                          1 Randomly Selected Samples                           \n",
      "                          ___________________________                           \n",
      "\n",
      "\n",
      "                                  Index = 2082                                  \n",
      "                                  ____________                                  \n",
      "                                   sale : 0\n",
      "                           sales_amount : nan\n",
      "                  conversion_time_delay : nan\n",
      "                               click_ts : 1,598,934,309.0\n",
      "                         n_clicks_1week : nan\n",
      "                          product_price : 0.0\n",
      "                      product_age_group : nan\n",
      "                            device_type : 7E56C27BFF0305E788DA55A029EC4988\n",
      "                            audience_id : 8186B349AF2B77A015F318DC49AC459B\n",
      "                         product_gender : nan\n",
      "                          product_brand : nan\n",
      "                     product_category_1 : nan\n",
      "                     product_category_2 : nan\n",
      "                     product_category_3 : nan\n",
      "                     product_category_4 : nan\n",
      "                     product_category_5 : nan\n",
      "                     product_category_6 : nan\n",
      "                     product_category_7 : nan\n",
      "                        product_country : 57A1D462A03BD076E029CF9310C11FC5\n",
      "                             product_id : 3D59B89C39707470E1199384A3CE8C7E\n",
      "                          product_title : nan\n",
      "                             partner_id : 7E3E579C9B4BDF1503E6F10F484334A6\n",
      "                                user_id : 532C579068D5338311184E8969140381\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.sample(1, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical variables have been hashed, but we can see this observation reflects a conversion, as indicated by the 1 in the sale column. The sale amount of 38 Euro suggests a discount off of the 44.99 Euro product price. Without a sense of the distributions, it would be challenging to derive inferences relating to the conversion time delay or the numbre of clicks during the prior week.  Let's alter the random state and select another from the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                                     greco                                      \n",
      "                          1 Randomly Selected Samples                           \n",
      "                          ___________________________                           \n",
      "\n",
      "\n",
      "                                  Index = 4941                                  \n",
      "                                  ____________                                  \n",
      "                             sale : 0\n",
      "                     sales_amount : nan\n",
      "            conversion_time_delay : nan\n",
      "                         click_ts : 1,598,889,781.0\n",
      "                   n_clicks_1week : 50.0\n",
      "                    product_price : 0.0\n",
      "                product_age_group : 921B36149E5B081FD24450BFE2CE4430\n",
      "                      device_type : 7E56C27BFF0305E788DA55A029EC4988\n",
      "                      audience_id : nan\n",
      "                   product_gender : A5D15FC386510762EC0DDFF54ABE6F94\n",
      "                    product_brand : AC9D783A521B93EE58930C47854D995C\n",
      "               product_category_1 : B4B45C6CE5FC4DE45AB02974D8849DBD\n",
      "               product_category_2 : A78952B21B4165ADA2189AE66887E2F1\n",
      "               product_category_3 : 8C94F9A181FAF330076CD12799059510\n",
      "               product_category_4 : 78B8CB2AC4295B9709C9305F5D1D7324\n",
      "               product_category_5 : nan\n",
      "               product_category_6 : nan\n",
      "               product_category_7 : nan\n",
      "                  product_country : D963E3BCE149E71F5D5E3000DCF68A9F\n",
      "                       product_id : FCE13A0753861ADEED4AF1F5BC20F23B\n",
      "                    product_title : 6337DCD4E46C4B8EE6CA417C5735AB4D B4D328F938979DB4D75B810750E3155D EE2BC4B713FB9D678F91954F1133E223 AC9D783A521B93EE58930C47854D995C\n",
      "                       partner_id : F122B91F6D102E4630817566839A4F1F\n",
      "                          user_id : 10E22C208255CDCD9371B05E7543E862\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.sample(1, random_state=random_state+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample reflects no conversion and fewer non-na data points in the record. Let's widen our aperture a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                                 Dataset greco                                  \n",
      "                                 _____________                                  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   sale                   10000 non-null  category\n",
      " 1   sales_amount           1612 non-null   float64 \n",
      " 2   conversion_time_delay  1608 non-null   float64 \n",
      " 3   click_ts               10000 non-null  float64 \n",
      " 4   n_clicks_1week         4700 non-null   float64 \n",
      " 5   product_price          10000 non-null  float64 \n",
      " 6   product_age_group      1815 non-null   category\n",
      " 7   device_type            9991 non-null   category\n",
      " 8   audience_id            2809 non-null   category\n",
      " 9   product_gender         1794 non-null   category\n",
      " 10  product_brand          2748 non-null   category\n",
      " 11  product_category_1     4786 non-null   category\n",
      " 12  product_category_2     4781 non-null   category\n",
      " 13  product_category_3     4290 non-null   category\n",
      " 14  product_category_4     2529 non-null   category\n",
      " 15  product_category_5     801 non-null    category\n",
      " 16  product_category_6     79 non-null     category\n",
      " 17  product_category_7     1 non-null      category\n",
      " 18  product_country        8457 non-null   category\n",
      " 19  product_id             8457 non-null   category\n",
      " 20  product_title          4758 non-null   object  \n",
      " 21  partner_id             10000 non-null  category\n",
      " 22  user_id                10000 non-null  category\n",
      "dtypes: category(17), float64(5), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains approximately 16m occupying nearly 2 GB of memory. Converting the pandas object variables to category data types may bring some computational efficiencies which may be material for a dataset of this size. Still, the number that stands out so far is the 45% rate of missing. Let's understand that a bit better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                             Missing Data Analysis                              \n",
      "                                 staging_greco                                  \n",
      "                             _____________________                              \n",
      "                           n  Missing  Missingness\n",
      "sale                   10000        0         0.00\n",
      "sales_amount           10000     8388        83.88\n",
      "conversion_time_delay  10000     8392        83.92\n",
      "click_ts               10000        0         0.00\n",
      "n_clicks_1week         10000     5300        53.00\n",
      "product_price          10000        0         0.00\n",
      "product_age_group      10000     8185        81.85\n",
      "device_type            10000        9         0.09\n",
      "audience_id            10000     7191        71.91\n",
      "product_gender         10000     8206        82.06\n",
      "product_brand          10000     7252        72.52\n",
      "product_category_1     10000     5214        52.14\n",
      "product_category_2     10000     5219        52.19\n",
      "product_category_3     10000     5710        57.10\n",
      "product_category_4     10000     7471        74.71\n",
      "product_category_5     10000     9199        91.99\n",
      "product_category_6     10000     9921        99.21\n",
      "product_category_7     10000     9999        99.99\n",
      "product_country        10000     1543        15.43\n",
      "product_id             10000     1543        15.43\n",
      "product_title          10000     5242        52.42\n",
      "partner_id             10000        0         0.00\n",
      "user_id                10000        0         0.00\n"
     ]
    }
   ],
   "source": [
    "_ = dataset.missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant sparsity is extant in the data. For about nine columns, over 1/2 the data are missing. with nine columns  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba6615e5a7b7e3657be37061ca9c48b0632f150281e10ebc62a462ee17016418"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('cvr': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
