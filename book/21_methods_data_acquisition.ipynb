{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "The Criteo Sponsored Search Conversion Log Dataset contains 90 days of live click and conversion traffic, twenty-three product features for over 16m observations. This preliminary segment will extract the data from the Criteo Labs website and stage it for analysis and downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from myst_nb import glue\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "from cvr.data import criteo_columns, criteo_dtypes\n",
    "from cvr.core.asset import AssetPassport\n",
    "from cvr.core.pipeline import DataPipelineBuilder, PipelineConfig\n",
    "from cvr.core.atelier import AtelierFabrique\n",
    "from cvr.utils.config import CriteoConfig \n",
    "from cvr.core.dataset import Dataset\n",
    "from cvr.data.io import DatasetWriter\n",
    "from cvr.data.etl import Extract, Transform\n",
    "from cvr.core.task import DatasetFactory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet, there are a few apriori data preprocessing technicalities worth addressing upfront as they will facilitate the profiling and analysis ahead. We'll add the column names, and convert the target variable 'sale', to a binary categorical variable. Non-numeric variables, currently coded as strings, will be convert to pandas category data types for computational and memory efficiencies. Finally, missing data are encoded with '-1'. We'll convert this missing value indicator to Pandas NA values for analysis and processing.\n",
    "\n",
    "## Data Pipeline\n",
    "To download, extract, and preprocess the data in a reproducible adjacent manner, we’ll construct a mini extract-transform-load (ETL) data pipeline. Once we extract the data from the Criteo Labs website, we'll persist the raw data, create a new dataset upon which we’ll perform the data preprocessing steps described above. Next, we'll create a Dataset object that exposes useful data profiling methods, then pickle the Dataset in a staging area for further data profiling and exploratry data analysis. Allons-y!\n",
    "\n",
    "As a preliminary step, let's establish a workspace, a studio, that will support experimentation and object persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'incept'\n",
    "description = 'là où tout commence'\n",
    "factory = AtelierFabrique()\n",
    "studio = factory.create(name=name, \n",
    "                        description=description, \n",
    "                        logging_level='info')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasource\n",
    "All data pipelines begin with a data source. The configuration details below provide the data source URL and local storage filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = CriteoConfig()\n",
    "source.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract\n",
    "Downloading the data the Criteo Labs site rate limits to approximately 5 Mbps. This is the most time-consuming step of the pipeline and can take upwards of 15 minutes of download time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passport = AssetPassport(\n",
    "    aid= studio.next_aid,\n",
    "    asset_type = 'task',\n",
    "    name = 'extract',\n",
    "    description = 'Extract step of the Criteo Data ETL pipeline',\n",
    "    stage = 'raw')\n",
    "\n",
    "extract = Extract(passport=passport,\n",
    "                  url=source.url,\n",
    "                  download_filepath=source.download_filepath,\n",
    "                  extract_filepath=source.extract_filepath,\n",
    "                  destination=source.destination,\n",
    "                  chunk_size=20,        # Download parameters\n",
    "                  n_groups=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "The Transform step will add column names, convert strings to category data types, replace the missing value indicators with NaNs and convert the target variable, 'sale', to a binary indicator data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passport = AssetPassport(\n",
    "    aid = studio.next_aid,\n",
    "    asset_type = 'task',\n",
    "    name = 'transform',\n",
    "    description = 'Transform step of the Criteo Data ETL pipeline',\n",
    "    stage = 'staged')\n",
    "\n",
    "transform = Transform(passport=passport,\n",
    "                      source=source.destination,\n",
    "                      colnames=criteo_columns,\n",
    "                      dtypes=criteo_dtypes,\n",
    "                      sep='\\t',\n",
    "                      missing_values=['-1',-1,-1.0,'-1.00',-1.00])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Save Dataset Object\n",
    "Create a Dataset object in the 'stage' stage for profiling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passport = AssetPassport(\n",
    "    aid = studio.next_aid,\n",
    "    asset_type = 'task',\n",
    "    name = 'create_dataset',\n",
    "    description = 'Create Dataset object of the Criteo Data ETL pipeline',\n",
    "    stage = 'staged')\n",
    "\n",
    "dataset_passport = AssetPassport(\n",
    "    aid = studio.next_aid,\n",
    "    asset_type = 'dataset',\n",
    "    name = 'criteo',\n",
    "    description = 'Criteo Staged Dataset Object',\n",
    "    stage = 'staged')\n",
    "\n",
    "dataset_factory = DatasetFactory(passport=passport, dataset_passport=dataset_passport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passport = AssetPassport(\n",
    "    aid = studio.next_aid,\n",
    "    asset_type = 'task',\n",
    "    name = 'save',\n",
    "    description = 'Save Dataset object for Criteo Data ETL pipeline',\n",
    "    stage = 'staged')\n",
    "\n",
    "\n",
    "dataset_writer = DatasetWriter(\n",
    "    passport=passport\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataPipeline Config and Construction\n",
    "Let's configure the pipeline with logging for progress monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipelineConfig(\n",
    "    logger=studio.logger,       # Logging object\n",
    "    verbose=True,               # Print messages\n",
    "    force=False,                # If step already completed, don't force it.\n",
    "    progress=False,              # No progress bar\n",
    "    dataset_repo=studio.assets,   # dataset repository\n",
    "    directory=studio.assets_directory   # Assets directory for the studio\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we construct the data pipeline and we are a 'go' for ignition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataPipelineBuilder()\n",
    "\n",
    "pipeline = builder.set_config(config).set_passport(passport).add_task(extract).add_task(transform).add_task(dataset_factory).add_task(dataset_writer).build().data_pipeline\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viola! Our dataset has been secured. Before we close this task, let's verify the pipeline endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = studio.get_asset(name='criteo', asset_type='dataset', stage='staged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtrain this dataset from the studio 'incept' by its asset id (aid) number '0003' or by its name, data_type, and stage. Data acquisition. This concludes the data acquisition segment."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba6615e5a7b7e3657be37061ca9c48b0632f150281e10ebc62a462ee17016418"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('cvr': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
