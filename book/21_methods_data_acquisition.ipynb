{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "Modeling the state-of-the-art from the 1970s, we'll obtain the Criteo Sponsored Search Conversion Log Dataset using the extract, transform and load (ETL) pattern. This will require a Criteo data source configuration, a series of tasks to perform the ETL operations and a pipeline to orchestrate the process. We'll import them now and briefly introduce the modules en route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvr.utils.config import CriteoConfig \n",
    "from cvr.data.etl import Extract, TransformETL, LoadDataset\n",
    "from cvr.core.pipeline import DataPipeline, DataPipelineBuilder\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "CriteoConfig packages the URL, file structure, and local destination file path information for the Criteo data source. For illustrative purposes, the CriteoConfig on this machine is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        Criteo Data Source Configuration                        \n",
      "                        ________________________________                        \n",
      "                          name : Criteo Sponsored Search Conversion Log Dataset\n",
      "                        source : http://go.criteo.net/criteo-research-search-conversion.tar.gz\n",
      "                   destination : data\\external\\criteo.tar.gz\n",
      "              filepath_extract : Criteo_Conversion_Search/CriteoSearchData\n",
      "                  filepath_raw : raw\\criteo.csv\n",
      "                     workspace : root\n",
      "                           sep : \\t\n",
      "                       missing : -1\n"
     ]
    }
   ],
   "source": [
    "config = CriteoConfig()\n",
    "config.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Steps\n",
    "Our three-step data pipeline involves the following: \n",
    "\n",
    "| Step | Module       | Description                                                                     |\n",
    "|------|--------------|---------------------------------------------------------------------------------|\n",
    "| 1    | Extract      | Downloads the source data into a local raw data directory                       |\n",
    "| 2    | TransformETL | Transform the raw data into a Dataset object and perform basic   preprocessing. |\n",
    "| 3    | LoadDataset  | Load the Dataset object into our local workspace.                               |\n",
    "\n",
    "Two basic preprocessing steps are taken a priori based upon the description of the data provided by Criteo Labs. First, we convert the missing values indicator (-1) to NaNs. Second, we convert non-numeric columns to the pandas' category data type for computational and space efficiency purposes. The tasks are instantiated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = Extract(config=config)\n",
    "transform=TransformETL(value=[-1,\"-1\"])\n",
    "load = LoadDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Builder\n",
    "Next, we'll construct the pipeline and place the resultant data in the staging area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataPipelineBuilder()\n",
    "builder.create()\n",
    "builder.set_name(\"vesuvio\").set_stage(\"staging\").set_force(True).set_keep_interim(True).set_verbose(True) \n",
    "builder.add_task(extract)\n",
    "builder.add_task(transform)\n",
    "builder.add_task(load)\n",
    "builder.build()\n",
    "pipeline = builder.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Execution\n",
    "The data are about 3.4 Gb compressed and nearly 6.5 Gb expanded; hence, downloading and loading the data are network and disk-intensive operations. This will take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started vesuvio\n",
      "\tDownloading 1910.08 Mb in 10 Mb chunks\n",
      "\n",
      "100%|██████████| 2.00G/2.00G [06:56<00:00, 4.81MiB/s]\n",
      "\t1910.081 Mb downloaded in 193 10 Mb chunks. Download complete!\n",
      "\tDecompression initiated.\n",
      "\tSampling dataset initiated.\n",
      "\tSampling Complete! 10000.0 Rows Sampled.\n",
      "\tDecompression Complete! 6129.08 Mb Extracted.\n",
      "Extract Complete. Status: 200: OK\n",
      "TransformETL Complete. Status: 200: OK\n",
      "LoadDataset Complete. Status: 200: OK\n",
      "Completed vesuvio. Duration 0:08:25.674205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                          DataPipeline vesuvio Summary                          \n",
      "                                  Extract Step                                  \n",
      "                          ____________________________                          \n",
      "                            Status Code : 200\n",
      "                           Content Type : application/x-gzip\n",
      "                          Last Modified : Wed, 08 Apr 2020 13:39:53 GMT\n",
      "                    Content Length (Mb) : 1,910.081\n",
      "                        Chunk Size (Mb) : 10\n",
      "                      Chunks Downloaded : 193\n",
      "                        Downloaded (Mb) : 1,910.081\n",
      "                         File Size (Mb) : 1,910.081\n",
      "                                   Mbps : 4.572\n",
      "                    Size Extracted (Mb) : 6,129.08\n",
      "           Sampled Dataset Observations : 10,000.0\n",
      "                   Sampled Dataset Size : 3.48\n",
      "                                  Start : 2022-01-24 01:26:00.489686\n",
      "                                    End : 2022-01-24 01:34:26.024576\n",
      "                               Duration : 0:08:25.534890\n",
      "                                 Status : 200: OK\n",
      "                            Status Date : 2022-01-24\n",
      "                            Status Time : 01:34:26\n",
      "\n",
      "\n",
      "                          DataPipeline vesuvio Summary                          \n",
      "                               TransformETL Step                                \n",
      "                          ____________________________                          \n",
      "                                  Start : 2022-01-24 01:34:26.031560\n",
      "                                    End : 2022-01-24 01:34:26.112860\n",
      "                               Duration : 0:00:00.081300\n",
      "                                 Status : 200: OK\n",
      "                            Status Date : 2022-01-24\n",
      "                            Status Time : 01:34:26\n",
      "\n",
      "\n",
      "                          DataPipeline vesuvio Summary                          \n",
      "                                LoadDataset Step                                \n",
      "                          ____________________________                          \n",
      "                               AID : staging_vesuvio\n",
      "                         Workspace : vesuvio\n",
      "                      Dataset Name : vesuvio\n",
      "                             Stage : staging\n",
      "                              name : vesuvio\n",
      "                          filepath : workspaces\\vesuvio\\Dataset\\staging\\vesuvio_Dataset_staging_vesuvio.pkl\n",
      "                             Start : 2022-01-24 01:34:26.115886\n",
      "                               End : 2022-01-24 01:34:26.156831\n",
      "                          Duration : 0:00:00.040945\n",
      "                            Status : 200: OK\n",
      "                       Status Date : 2022-01-24\n",
      "                       Status Time : 01:34:26\n",
      "\n",
      "\n",
      "                          DataPipeline vesuvio Summary                          \n",
      "                          ____________________________                          \n",
      "                                Extract : 200: OK\n",
      "                           TransformETL : 200: OK\n",
      "                            LoadDataset : 200: OK\n",
      "                                  Start : 2022-01-24 01:26:00.486630\n",
      "                                    End : 2022-01-24 01:34:26.160835\n",
      "                               Duration : 0:08:25.674205\n"
     ]
    }
   ],
   "source": [
    "pipeline.run()\n",
    "pipeline.summary"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba6615e5a7b7e3657be37061ca9c48b0632f150281e10ebc62a462ee17016418"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('cvr': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
