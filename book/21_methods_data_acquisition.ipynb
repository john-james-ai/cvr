{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "Extract, transform, load (ETL), popularized in the ’70s, is a design pattern for continuous extraction of data from multiple heterogeneous data sources, enforcement of strict data quality and consistency standards, and finally, the presentation of massive, transformed, normalized, and sanitized data into enterprise-level data warehouses and (or) data lakes, for analysis, business intelligence, and analytics. Though this project has none of those things, we’ll adopt ETL as our organizing framework for the data acquisition phase.\n",
    "\n",
    "## Extract Transform Load (ETL) Overview\n",
    "To implement the ETL process, we'll need the data source configuration, a pipeline to orchestrate the process, and the tasks to perform the operations on the data. Let's import these assets here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvr.core.pipeline import DataPipeline, DataPipelineBuilder, PipelineCommand\n",
    "from cvr.data.etl import Download, ExtractRawData, ConvertDtypes, SetNA, LoadData\n",
    "from cvr.utils.config import CriteoConfig "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source Configuration\n",
    "We start with the Criteo configuration object, CriteoConfig. It contains the data source parameters such as the download url, the local file path, the structure of the gzip archive and so on. Methods are exposed for each of the tasks described below. The configuration on this machine is shown for illustrative purposes. \n",
    "\n",
    "### ETL Pipeline\n",
    "The ETL process is orchestrated by the DataPipeline, constructed by a DataPipelineBuilder, and parameterized by the PipelineCommand object.\n",
    "\n",
    "| # | Class               | Description                                               |\n",
    "|---|---------------------|-----------------------------------------------------------|\n",
    "| 1 | DataPipeline        | Executes the series of extract, transform and load tasks. |\n",
    "| 2 | DataPipelineBuilder | Constructs the DataPipeline object                        |\n",
    "| 3 | PipelineCommand     | Encapsulates the parameters of the DataPipeline           |\n",
    "\n",
    "### ETL Pipeline Tasks\n",
    "Finally, we have the tasks that perform the operations.\n",
    "\n",
    "| Step | Phase     | Class          | Description                                                     |\n",
    "|------|-----------|----------------|-----------------------------------------------------------------|\n",
    "| 1    | Extract   | Download       | Downloads the source data into a local directory                |\n",
    "| 2    | Extract   | ExtractRawData | Extract the raw data from the gzip file                         |\n",
    "| 3    | Transform | ConvertDtypes  | Converts the target and object variables to category data type. |\n",
    "| 4    | Transform | SetNA          | Changes the missing value indicator, '-1', to NaNs.             |\n",
    "| 5    | Load      | LoadData       | Loads the preprocessed data into a staging directory.           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source Configuration\n",
    "We start with the Criteo configuration object, CriteoConfig. It contains the data source parameters such as the download url, the local file path, the structure of the gzip archive and so on. Methods are exposed for each of the tasks described below. The configuration on this machine is shown for illustrative purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        Criteo Data Source Configuration                        \n",
      "                        ________________________________                        \n",
      "                           name : Criteo Sponsored Search Conversion Log Dataset\n",
      "                download_source : http://go.criteo.net/criteo-research-search-conversion.tar.gz\n",
      "           download_destination : data/criteo/external/criteo.tar.gz\n",
      "                 extract_source : data/criteo/external/criteo.tar.gz\n",
      "            extract_destination : data/criteo/raw/criteo.csv\n",
      "               extract_filepath : Criteo_Conversion_Search/CriteoSearchData\n",
      "                 convert_source : data/criteo/raw/criteo.csv\n",
      "               load_destination : data/criteo/staged/criteo.csv\n",
      "                      workspace : root\n",
      "                            sep : \\t\n",
      "                        missing : -1\n"
     ]
    }
   ],
   "source": [
    "config_filepath = \"tests\\\\test_config\\criteo.yaml\"\n",
    "config = CriteoConfig()\n",
    "config.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Pipeline Tasks\n",
    "Next, we have the pipeline tasks.\n",
    "\n",
    "| Step | Phase     | Class          | Description                                                     |\n",
    "|------|-----------|----------------|-----------------------------------------------------------------|\n",
    "| 1    | Extract   | Download       | Downloads the source data into a local directory                |\n",
    "| 2    | Extract   | ExtractRawData | Extract the raw data from the gzip file                         |\n",
    "| 3    | Transform | ConvertDtypes  | Converts the target and object variables to category data type. |\n",
    "| 4    | Transform | SetNA          | Changes the missing value indicator, '-1', to NaNs.             |\n",
    "| 5    | Load      | LoadData       | Loads the preprocessed data into a staging directory.           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "download = Download(source=config.download_source, destination=config.download_destination)\n",
    "extract = ExtractRawData(source=config.extract_source, \n",
    "                         destination=config.extract_destination, \n",
    "                         filepath_extract=config.extract_filepath)\n",
    "\n",
    "# Transform\n",
    "convert = ConvertDtypes(source=config.convert_source)\n",
    "setna = SetNA(value=[-1,\"-1\"])\n",
    "\n",
    "# Load\n",
    "load = LoadData(destination=config.load_destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Pipeline Development\n",
    "Next, we create the PipelineCommand which parameterizes the DataPipeline object. The force parameter indicates whether a step should be executed if the output data already exists and verbose specifies whether reporting will be provided during pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = PipelineCommand(name=\"Criteo ETL\", force=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we construct the DataPipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataPipelineBuilder()\n",
    "builder.create(command)\n",
    "# Extract\n",
    "builder.add_task(download)\n",
    "builder.add_task(extract)\n",
    "# Transform\n",
    "builder.add_task(convert)\n",
    "builder.add_task(setna)\n",
    "# Load\n",
    "builder.add_task(load)\n",
    "\n",
    "builder.build()\n",
    "etl = builder.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Pipeline Execution\n",
    "The pipeline is ready to run. Dataset size: 6.5 Gb. Estimated processing time: 15 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:23:22,607 - root_pipeline - INFO - Started root_pipeline\n",
      "2022-01-21 15:23:22,608 - root_pipeline - INFO - Started Task: Download\n",
      "2022-01-21 15:23:23,680 - root_pipeline - INFO - \tDownloading 1910.08 Mb\n",
      "\n",
      "2022-01-21 15:23:46,973 - root_pipeline - INFO - \tChunk #10: 5.24 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:24:11,447 - root_pipeline - INFO - \tChunk #20: 10.47 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:24:34,605 - root_pipeline - INFO - \tChunk #30: 15.71 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:24:57,618 - root_pipeline - INFO - \tChunk #40: 20.94 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:25:20,783 - root_pipeline - INFO - \tChunk #50: 26.18 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:25:43,924 - root_pipeline - INFO - \tChunk #60: 31.41 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:26:06,523 - root_pipeline - INFO - \tChunk #70: 36.65 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:26:29,132 - root_pipeline - INFO - \tChunk #80: 41.88 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:26:52,538 - root_pipeline - INFO - \tChunk #90: 47.12 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:27:15,492 - root_pipeline - INFO - \tChunk #100: 52.35 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:27:38,821 - root_pipeline - INFO - \tChunk #110: 57.59 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:28:01,559 - root_pipeline - INFO - \tChunk #120: 62.82 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:28:25,083 - root_pipeline - INFO - \tChunk #130: 68.06 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:28:48,240 - root_pipeline - INFO - \tChunk #140: 73.3 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:29:11,189 - root_pipeline - INFO - \tChunk #150: 78.53 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:29:34,632 - root_pipeline - INFO - \tChunk #160: 83.77 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:29:57,444 - root_pipeline - INFO - \tChunk #170: 89.0 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:30:20,694 - root_pipeline - INFO - \tChunk #180: 94.24 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:30:43,680 - root_pipeline - INFO - \tChunk #190: 99.47 percent downloaded at 4 Mbps\n",
      "2022-01-21 15:30:45,996 - root_pipeline - INFO - Ended Task: Download. Status: 200: OK\n",
      "2022-01-21 15:30:46,000 - root_pipeline - INFO - Started Task: ExtractRawData\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        DataPipeline Criteo ETL Summary                         \n",
      "                                 Download Step                                  \n",
      "                        _______________________________                         \n",
      "                            Status Code : 200\n",
      "                           Content Type : application/x-gzip\n",
      "                          Last Modified : Wed, 08 Apr 2020 13:39:53 GMT\n",
      "                    Content Length (Mb) : 1,910.081\n",
      "                        Chunk Size (Mb) : 10\n",
      "                      Chunks Downloaded : 193\n",
      "                        Downloaded (Mb) : 1,910.081\n",
      "                         File Size (Mb) : 1,910.081\n",
      "                                   Mbps : 4.308\n",
      "                                  Start : 2022-01-21 15:23:22.610763\n",
      "                                    End : 2022-01-21 15:30:45.996134\n",
      "                               Duration : 0:07:23.385371\n",
      "                                 Status : 200: OK\n",
      "                            Status Date : 2022-01-21\n",
      "                            Status Time : 15:30:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:32:41,664 - root_pipeline - INFO - Ended Task: ExtractRawData. Status: 200: OK\n",
      "2022-01-21 15:32:41,669 - root_pipeline - INFO - Started Task: ConvertDtypes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        DataPipeline Criteo ETL Summary                         \n",
      "                              ExtractRawData Step                               \n",
      "                        _______________________________                         \n",
      "                                 Source : data/criteo/external/criteo.tar.gz\n",
      "                        Compressed Size : 2,002,864,638\n",
      "                            Destination : data/criteo/raw/criteo.csv\n",
      "                          Expanded Size : 6,426,808,162\n",
      "                                  Start : 2022-01-21 15:30:46.000168\n",
      "                                    End : 2022-01-21 15:32:41.664393\n",
      "                               Duration : 0:01:55.664225\n",
      "                                 Status : 200: OK\n",
      "                            Status Date : 2022-01-21\n",
      "                            Status Time : 15:32:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:37:25,359 - root_pipeline - INFO - Ended Task: ConvertDtypes. Status: 200: OK\n",
      "2022-01-21 15:37:25,363 - root_pipeline - INFO - Started Task: SetNA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        DataPipeline Criteo ETL Summary                         \n",
      "                               ConvertDtypes Step                               \n",
      "                        _______________________________                         \n",
      "                                   Rows : 15,995,634\n",
      "                                Columns : 23\n",
      "                                   Size : 5,380,123,480\n",
      "                                Missing : 65,417\n",
      "                        Rows w/ Missing : 65,417\n",
      "                     Columns w/ Missing : 1\n",
      "                               category : 17\n",
      "                                  int64 : 3\n",
      "                                float64 : 2\n",
      "                                 object : 1\n",
      "                                  Start : 2022-01-21 15:32:41.669787\n",
      "                                    End : 2022-01-21 15:37:25.359735\n",
      "                               Duration : 0:04:43.689948\n",
      "                                 Status : 200: OK\n",
      "                            Status Date : 2022-01-21\n",
      "                            Status Time : 15:37:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:37:40,866 - root_pipeline - INFO - Ended Task: SetNA. Status: 200: OK\n",
      "2022-01-21 15:37:40,868 - root_pipeline - INFO - Started Task: LoadData\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        DataPipeline Criteo ETL Summary                         \n",
      "                                   SetNA Step                                   \n",
      "                        _______________________________                         \n",
      "                                   Rows : 15,995,634\n",
      "                                Columns : 23\n",
      "                                  Cells : 367,899,582\n",
      "                         Missing Before : 65,417\n",
      "                       Missing % Before : 0.02\n",
      "                          Missing After : 167,746,417\n",
      "                        Missing % After : 45.6\n",
      "                               % Change : 256,326.34\n",
      "                                  Start : 2022-01-21 15:37:25.363766\n",
      "                                    End : 2022-01-21 15:37:40.866263\n",
      "                               Duration : 0:00:15.502497\n",
      "                                 Status : 200: OK\n",
      "                            Status Date : 2022-01-21\n",
      "                            Status Time : 15:37:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:37:59,855 - root_pipeline - INFO - Ended Task: LoadData. Status: 200: OK\n",
      "2022-01-21 15:37:59,857 - root_pipeline - INFO - Completed root_pipeline. Duration 0:14:37.249498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        DataPipeline Criteo ETL Summary                         \n",
      "                                 LoadData Step                                  \n",
      "                        _______________________________                         \n",
      "                            Destination : data/criteo/staged/criteo.csv\n",
      "                         File Size (Mb) : 1,835.47\n",
      "                                  Start : 2022-01-21 15:37:40.868256\n",
      "                                    End : 2022-01-21 15:37:59.855772\n",
      "                               Duration : 0:00:18.987516\n",
      "                                 Status : 200: OK\n",
      "                            Status Date : 2022-01-21\n",
      "                            Status Time : 15:37:59\n",
      "\n",
      "\n",
      "                        DataPipeline Criteo ETL Summary                         \n",
      "                        _______________________________                         \n",
      "                               Download : 200: OK\n",
      "                         ExtractRawData : 200: OK\n",
      "                          ConvertDtypes : 200: OK\n",
      "                                  SetNA : 200: OK\n",
      "                               LoadData : 200: OK\n",
      "                                  Start : 2022-01-21 15:23:22.607796\n",
      "                                    End : 2022-01-21 15:37:59.857294\n",
      "                               Duration : 0:14:37.249498\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23532\\3015933591.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0metl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0metl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "etl.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba6615e5a7b7e3657be37061ca9c48b0632f150281e10ebc62a462ee17016418"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('cvr': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
