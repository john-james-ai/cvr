{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "Modeling the state-of-the-art from the 1970s, we'll obtain the data using the standard extract, transform and load (ETL) design pattern.  This will require a Criteo data source configuration, a series of tasks to perform the ETL operations, a pipeline to orchestrate the process, and a workspace into which the output Dataset object will be loaded. We'll import them now and introduce the modules enroute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-output",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from myst_nb import glue\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "from cvr.core.dataset import DatasetRequest\n",
    "from cvr.core.workspace import Workspace, WorkspaceManager\n",
    "from cvr.utils.config import CriteoConfig \n",
    "from cvr.data.etl import Extract, TransformETL, LoadDataset\n",
    "from cvr.core.pipeline import DataPipeline, DataPipelineBuilder, DataPipelineRequest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace\n",
    "Before we set up the ETL pipeline, we will need to establish a workspace for it. A Workspace object is essentially a container for Datasets, Models, and Experiments. Singleton class WorkspaceManager allows us to create and manage Workspace objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE\n",
    "# wsm = WorkspaceManager()\n",
    "# wsm.delete_workspace('Trieste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsm = WorkspaceManager()\n",
    "wsm.set_current_workspace(name=\"Vesuvio\")\n",
    "vesuvio = wsm.get_current_workspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workspace Trieste has been created and is now the 'current' workspace. Datasets, Models, and Experiments will be contained within this workspace until another workspace is created and/or set current."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "The CriteoConfig object packages the URL, file structure, and local destination file path information for the Criteo data source. For illustrative purposes, the CriteoConfig on this machine is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                        Criteo Data Source Configuration                        \n",
      "                        ________________________________                        \n",
      "                         name : Criteo Sponsored Search Conversion Log Dataset\n",
      "                          url : http://go.criteo.net/criteo-research-search-conversion.tar.gz\n",
      "                  destination : data\\external\\criteo.tar.gz\n",
      "             filepath_extract : Criteo_Conversion_Search/CriteoSearchData\n",
      "                 filepath_raw : raw\\criteo.csv\n",
      "                          sep : \\t\n",
      "                      missing : -1\n"
     ]
    }
   ],
   "source": [
    "config = CriteoConfig()\n",
    "config.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Steps\n",
    "Our pipeline consists of three steps, Extract, TransformETL and LoadDataset described below. \n",
    "\n",
    "| Step | Module       | Description                                                                     |\n",
    "|------|--------------|---------------------------------------------------------------------------------|\n",
    "| 1    | Extract      | Downloads the source data into a local raw data directory                       |\n",
    "| 2    | TransformETL | Transforms the raw data into a Dataset object and performs basic preprocessing. |\n",
    "| 3    | LoadDataset  | Loads the Dataset object into our current workspace.                            |\n",
    "\n",
    "The Extract step takes the CriteoConfig data source configuration object as input and produces our raw data. Transform ETL performs two basic preprocessing a priori based upon the description of the data provided by Criteo Labs. First, we convert the missing values indicator (-1) to NaNs. Second, we convert the categorical variables to the pandas' category data type for computation and space efficiency purposes. The pipeline tasks are instantiated below. Finally, LoadDataset loads the preprocessed Dataset object into our current workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = Extract(datasource_config=config, chunk_size=20)\n",
    "transform=TransformETL(value=[-1,\"-1\"])\n",
    "load = LoadDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Request\n",
    "Specifying the parameters for the Pipeline and its resultant Dataset is performed via request objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_request = DatasetRequest(name=\"criteo\", \n",
    "                                 description=\"Criteo Preprocessed\", \n",
    "                                 stage=\"preprocessed\", \n",
    "                                 sample_size=None,\n",
    "                                 )\n",
    "pipeline_request = DataPipelineRequest(name=\"etl\", \n",
    "                                       stage=\"preprocessed\", \n",
    "                                       workspace=vesuvio,\n",
    "                                       random_state=602, \n",
    "                                       force=True, \n",
    "                                       logging_level='info',\n",
    "                                       dataset_request=dataset_request\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Builder\n",
    "Now, we pass our request to the DataPipelineBuilder object, add the tasks, and call the build method. The pipeline is provided via a property on the builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DataPipelineBuilder()\n",
    "builder.reset()\n",
    "builder.make_request(pipeline_request) \n",
    "builder.add_task(extract)\n",
    "builder.add_task(transform)\n",
    "builder.add_task(load)\n",
    "builder.build()\n",
    "pipeline = builder.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ETL Pipeline Execution\n",
    "The dataset is approximately 6.5 GB; making this ETL a network and IO intensive process. Estimated processing time: 12 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started etl\n",
      "\tDecompression initiated.\n",
      "\tDecompression Complete! 6129.08 Mb Extracted.\n",
      "Completed etl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                              Pipeline etl Summary                              \n",
      "                              ____________________                              \n",
      "           Task                      Start                        End  Minutes   Status\n",
      "0       Extract 2022-01-30 12:00:46.344152 2022-01-30 12:07:26.794839     6.67  200: OK\n",
      "1  TransformETL 2022-01-30 12:07:26.797846 2022-01-30 12:07:42.548274     0.26  200: OK\n",
      "2   LoadDataset 2022-01-30 12:07:42.551274 2022-01-30 12:08:06.639556     0.40  200: OK\n"
     ]
    }
   ],
   "source": [
    "dataset = pipeline.run()\n",
    "pipeline.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline appears to have run successfully. Let's check the task summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                              Extract Task Summary                              \n",
      "                       Dataset: Etl / Preprocessed Stage                        \n",
      "                       _________________________________                        \n",
      "                    Size Extracted (Mb) : 6,129.08\n",
      "                                  Start : 2022-01-30 12:00:46.344152\n",
      "                                    End : 2022-01-30 12:07:26.794839\n",
      "                               Duration : 0:06:40.450687\n",
      "                                 Status : 200: OK\n",
      "                            Status Date : 2022-01-30\n",
      "                            Status Time : 12:07:26\n"
     ]
    }
   ],
   "source": [
    "xsum = extract.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/papermill.record/text/plain": "1910.081"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "etl_downloaded"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "20"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "etl_chunk_size"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "97"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "etl_chunks_downloaded"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "5.102"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "etl_speed"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "6129.08"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "etl_size"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "6.67"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "etl_duration"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GLUE\n",
    "_ = glue(\"etl_downloaded\",xsum.get(\"Content Length (Mb)\",1910.081), display=False)\n",
    "_ = glue(\"etl_chunk_size\", xsum.get(\"Chunk Size (Mb)\", 20), display=False)\n",
    "_ = glue(\"etl_chunks_downloaded\", xsum.get(\"Chunks Downloaded\",97), display=False)\n",
    "_ = glue(\"etl_speed\", xsum.get(\"Mbps\",5.102), display=False)\n",
    "_ = glue(\"etl_size\", xsum.get(\"Size Extracted (Mb)\",6129.08), display=False)\n",
    "if 'Duration' in xsum.keys():\n",
    "    _ = glue(\"etl_duration\", round(xsum[\"Duration\"].total_seconds() / 60,2), display=False)\n",
    "else:\n",
    "    _ = glue(\"etl_duration\",17.38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that we've downloaded {glue:}`etl_downloaded` Mb in about {glue:}`etl_duration` minutes in {glue:}`etl_chunks_downloaded`  {glue:}`etl_chunk_size` Mb chunks with an average throughput of {glue:}`etl_speed` Mbps. Next, we have the transform step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                           TransformETL Task Summary                            \n",
      "          Missing Values Replacement: Dataset Etl / Preprocessed Stage          \n",
      "          ____________________________________________________________          \n",
      "                       Before     After\n",
      "sale                        0         0\n",
      "sales_amount                0  14262913\n",
      "conversion_time_delay       0  14268293\n",
      "click_ts                    0         0\n",
      "n_clicks_1week              0   6744427\n",
      "product_price               0         0\n",
      "product_age_group           0  11760058\n",
      "device_type                 0      3032\n",
      "audience_id                 0  11502018\n",
      "product_gender              0  11654195\n",
      "product_brand               0   7241560\n",
      "product_category_1          0   6142756\n",
      "product_category_2          0   6151249\n",
      "product_category_3          0   7342676\n",
      "product_category_4          0  10494308\n",
      "product_category_5          0  14595054\n",
      "product_category_6          0  15717150\n",
      "product_category_7          0  15995414\n",
      "product_country             0   3834802\n",
      "product_id                  0   3828338\n",
      "product_title           65417   6208174\n",
      "partner_id                  0         0\n",
      "user_id                     0         0\n",
      "\n",
      "\n",
      "                           TransformETL Task Summary                            \n",
      "                                                                                \n",
      "                           _________________________                            \n",
      "          Missing      Total   Pct\n",
      "Before      65417  367899582  0.02\n",
      "After   167746417  367899582 45.60\n"
     ]
    }
   ],
   "source": [
    "_ = transform.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing the missing value indicators with NaNs will simplify data processing and analysis. It does reveal; however, signficant data sparsity. Notably, diversity and sparsity in observations are common challenges in marketing and customer analytics. \n",
    "\n",
    "Lastly, we have the load step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                            LoadDataset Task Summary                            \n",
      "                       Dataset: Etl / Preprocessed Stage                        \n",
      "                       _________________________________                        \n",
      "                             AID : preprocessed_criteo_013022\n",
      "                       Workspace : Vesuvio\n",
      "                    Dataset Name : criteo\n",
      "                           Stage : preprocessed\n",
      "                        filepath : workspaces\\Vesuvio\\datasets\\preprocessed\\dataset_preprocessed_preprocessed_criteo_013022.pkl\n",
      "                           Start : 2022-01-30 12:07:42.551274\n",
      "                             End : 2022-01-30 12:08:06.639556\n",
      "                        Duration : 0:00:24.088282\n",
      "                          Status : 200: OK\n",
      "                     Status Date : 2022-01-30\n",
      "                     Status Time : 12:08:06\n"
     ]
    }
   ],
   "source": [
    "_ = load.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll note the name and stage for this dataset, which we will use to obtain the Dataset object from the workspace. Before closing this section, we'll demonstrate how an object can be stored and retrieved from our current workspace, 'vesuvio'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvr.data.profile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21436\\1827311162.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvesuvio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'criteo'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'preprocessed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\John\\Documents\\Data Science\\Projects\\cvr\\cvr\\core\\workspace.py\u001b[0m in \u001b[0;36mget_dataset\u001b[1;34m(self, stage, name)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdelete_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\John\\Documents\\Data Science\\Projects\\cvr\\cvr\\core\\asset.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, stage, name)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No filepath found for {} {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\John\\Documents\\Data Science\\Projects\\cvr\\cvr\\utils\\io.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, filepath)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mpicklefile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0masset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpicklefile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mpicklefile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0masset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cvr.data.profile'"
     ]
    }
   ],
   "source": [
    "dataset = vesuvio.get_dataset(name='criteo', stage='preprocessed')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viola! This closes the data acquisition portion of this series. In the next section, we will get our first glimpses of the data from a profiling and data quality perspective."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba6615e5a7b7e3657be37061ca9c48b0632f150281e10ebc62a462ee17016418"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('cvr': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
